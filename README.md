<p align="center">
  <h1 align="center">ğŸ§  Advanced RAG System</h1>
  <p align="center">
    <strong>Production-grade Retrieval-Augmented Generation with LangChain orchestration & HuggingFace models</strong>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/LangChain-LCEL_Pipeline-blue?logo=langchain" alt="LangChain"/>
    <img src="https://img.shields.io/badge/HuggingFace-Inference_API-yellow?logo=huggingface" alt="HuggingFace"/>
    <img src="https://img.shields.io/badge/LLM-Llama_3.1_8B-purple" alt="Llama"/>
    <img src="https://img.shields.io/badge/VectorDB-ChromaDB-green" alt="ChromaDB"/>
    <img src="https://img.shields.io/badge/API-FastAPI-009688?logo=fastapi" alt="FastAPI"/>
    <img src="https://img.shields.io/badge/LLMOps-Eval_%2B_Tuning-red" alt="LLMOps"/>
  </p>
</p>

---

## ğŸ—ï¸ System Architecture

> Full LLMOps pipeline: **Ingest â†’ Index â†’ Retrieve â†’ Generate â†’ Evaluate â†’ Tune**

```mermaid
graph TB
    subgraph INGESTION["ğŸ“¥ Data Ingestion Pipeline"]
        direction TB
        A1["ğŸ“„ Documents<br/>(PDF Â· DOCX Â· TXT)"]
        A2["ğŸ”— LangChain Loaders<br/><i>PyPDFLoader Â· TextLoader Â· Docx2txtLoader</i>"]
        A3["âœ‚ï¸ LangChain Text Splitters<br/><i>RecursiveCharacterTextSplitter</i><br/>chunk_size=512 Â· overlap=50"]
        A1 --> A2 --> A3
    end

    subgraph INDEXING["ğŸ—„ï¸ Dual Indexing"]
        direction TB
        B1["ğŸ§  HuggingFace Embeddings<br/><i>BAAI/bge-small-en-v1.5</i><br/>LangChain HuggingFaceEmbeddings"]
        B2["ğŸ“Š ChromaDB VectorStore<br/><i>LangChain Chroma</i><br/>384-dim dense vectors"]
        B3["ğŸ“ BM25 Sparse Index<br/><i>LangChain BM25Retriever</i><br/>TF-IDF keyword index"]
        A3 --> B1 --> B2
        A3 --> B3
    end

    subgraph RETRIEVAL["ğŸ” Hybrid Retrieval"]
        direction TB
        C1["ğŸ¯ Dense Retrieval<br/><i>Chroma.as_retriever()</i>"]
        C2["ğŸ“‹ Sparse Retrieval<br/><i>BM25Retriever</i>"]
        C3["âš¡ EnsembleRetriever<br/><i>Reciprocal Rank Fusion</i><br/>dense=0.7 Â· sparse=0.3"]
        C4["ğŸ† CrossEncoder Reranker<br/><i>ContextualCompressionRetriever</i><br/>ms-marco-MiniLM-L-6-v2"]
        B2 --> C1
        B3 --> C2
        C1 --> C3
        C2 --> C3
        C3 --> C4
    end

    subgraph GENERATION["ğŸ¤– LLM Generation â€” LCEL Chain"]
        direction TB
        D1["ğŸ“ ChatPromptTemplate"]
        D2["ğŸ¦™ HuggingFace Inference API<br/><i>ChatHuggingFace + HuggingFaceEndpoint</i><br/>meta-llama/Llama-3.1-8B-Instruct"]
        D3["ğŸ“¤ StrOutputParser"]
        D4["ğŸ’¬ Response"]
        C4 --> D1 --> D2 --> D3 --> D4
    end

    subgraph EVAL["ğŸ“Š LLMOps â€” Evaluation & Tuning"]
        direction LR
        E1["ğŸ¯ Retrieval<br/>P@K Â· R@K Â· MRR"]
        E2["âœ… Generation<br/>Faithfulness Â· Hallucination"]
        E3["ğŸ”§ Param Tuning<br/>Grid Search"]
        E4["ğŸ“ˆ Baseline<br/>Tracking"]
    end

    subgraph SERVE["ğŸŒ Serving Layer"]
        direction LR
        F1["âš¡ FastAPI"]
        F2["â¤ï¸ Health"]
        F3["ğŸ“š /docs"]
    end

    D4 --> F1
    RETRIEVAL --> E1
    GENERATION --> E2
    E1 --> E3
    E2 --> E3
    E3 --> E4

    Q["ğŸ‘¤ Query"] --> C1
    Q --> C2

    style INGESTION fill:#1a1a2e,stroke:#e94560,color:#fff
    style INDEXING fill:#16213e,stroke:#0f3460,color:#fff
    style RETRIEVAL fill:#0f3460,stroke:#533483,color:#fff
    style GENERATION fill:#533483,stroke:#e94560,color:#fff
    style EVAL fill:#2d2d44,stroke:#e94560,color:#fff
    style SERVE fill:#1a1a2e,stroke:#0f3460,color:#fff
```

### LCEL Chain Pattern (core generation pipeline)

```python
# LangChain Expression Language â€” composable, streamable, observable
chain = (
    {"context": retriever | format_docs, "query": RunnablePassthrough()}
    | ChatPromptTemplate(system + context + query)
    | ChatHuggingFace(HuggingFaceEndpoint("meta-llama/Llama-3.1-8B-Instruct"))
    | StrOutputParser()
)
answer = chain.invoke("What is retrieval-augmented generation?")
```

---

## ğŸ§© LangChain & HuggingFace Components Map

Every module in this project maps directly to a **LangChain** component, with **HuggingFace** models powering embeddings, reranking, and generation:

| Module | LangChain Component | HuggingFace Model |
|--------|--------------------|--------------------|
| `src/data/loader.py` | `PyPDFLoader` Â· `TextLoader` Â· `Docx2txtLoader` | â€” |
| `src/data/processor.py` | `RecursiveCharacterTextSplitter` | â€” |
| `src/retrieval/embeddings.py` | `HuggingFaceEmbeddings` | **BAAI/bge-small-en-v1.5** |
| `src/retrieval/vector_store.py` | `langchain_chroma.Chroma` | â€” |
| `src/retrieval/bm25_retriever.py` | `BM25Retriever` | â€” |
| `src/retrieval/hybrid_retriever.py` | `EnsembleRetriever` (Reciprocal Rank Fusion) | â€” |
| `src/retrieval/reranker.py` | `ContextualCompressionRetriever` + `CrossEncoderReranker` | **cross-encoder/ms-marco-MiniLM-L-6-v2** |
| `src/generation/llm.py` | `ChatHuggingFace` + `HuggingFaceEndpoint` | **meta-llama/Llama-3.1-8B-Instruct** |
| `src/generation/chain.py` | LCEL: `RunnablePassthrough` â†’ `ChatPromptTemplate` â†’ `StrOutputParser` | â€” |
| `src/generation/prompts.py` | `ChatPromptTemplate.from_messages()` | â€” |
| `src/api/main.py` | FastAPI serving layer | â€” |

### LLM Backend (dual support)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM_BACKEND=huggingface (default)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ HuggingFaceEndpointâ”‚â”€â”€â”€â–¶â”‚ ChatHuggingFace              â”‚  â”‚
â”‚  â”‚ (Inference API)   â”‚    â”‚ meta-llama/Llama-3.1-8B      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LLM_BACKEND=ollama (fallback)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Ollama Server     â”‚â”€â”€â”€â–¶â”‚ ChatOllama                    â”‚  â”‚
â”‚  â”‚ (localhost:11434) â”‚    â”‚ llama3.1:8b                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Quick Start

```bash
# 1. Clone & setup
git clone https://github.com/AMINAATTAOUI/advanced-rag-system.git
cd advanced-rag-system
python -m venv .venv && .venv\Scripts\activate  # Windows
# source .venv/bin/activate                     # Linux/Mac

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure HuggingFace API
cp .env.example .env
# Edit .env â†’ add your HF_API_TOKEN from https://huggingface.co/settings/tokens
# Free tier is sufficient for inference

# 4. Download & index documents (example: ArXiv papers)
python scripts/download_data.py --num-papers 50
python scripts/build_index.py

# 5. Verify the full pipeline
python tests/test_system.py

# 6. Start API server
python -m uvicorn src.api.main:app --reload --port 8000
# â†’ http://localhost:8000/docs
```

---

## ğŸ“Š LLMOps â€” Evaluation & Baseline Metrics

A core LLMOps practice: **measure before you tune**. This system includes a full evaluation suite with baseline tracking and regression detection.

### Achieved Baseline (50 PDFs, 20 eval queries)

| Category | Metric | Score | Status |
|----------|--------|-------|--------|
| **Retrieval** | Precision@5 | **0.850** | âœ… EXCELLENT |
| | Recall@5 | **0.900** | âœ… EXCELLENT |
| | MRR | **0.900** | âœ… EXCELLENT |
| | Avg Latency | 49ms (p95: 66ms) | âš¡ FAST |
| **Generation** | Faithfulness | **0.975** | âœ… EXCELLENT |
| | Answer Relevance | **0.824** | âœ… GOOD |
| | Context Precision | **0.790** | âœ… GOOD |
| | Hallucination Rate | **0.025** | âœ… EXCELLENT |
| **System** | Overall | **PRODUCTION READY** | âœ… |

### Evaluation Commands

```bash
# Step 1: Capture baseline (run after indexing)
python tests/test_baseline_metrics.py --dataset_size 50 --save_results

# Step 2: Evaluate retrieval (compare dense vs sparse vs hybrid)
python tests/test_evaluate_retrieval.py --method hybrid --top_k 5
python tests/test_evaluate_retrieval.py --method dense --top_k 5
python tests/test_evaluate_retrieval.py --method sparse --top_k 5

# Step 3: Evaluate generation quality
python tests/test_evaluate_generation.py --method hybrid --num_queries 20

# Step 4: Parameter tuning (LLMOps iterative optimization)
python scripts/tune_parameters.py --param chunk_size --values 256,512,1024
python scripts/failure_analysis.py --threshold 0.6
python tests/test_score_thresholds.py --thresholds 0.3,0.5,0.7
```

### LLMOps Workflow

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Baseline â”‚â”€â”€â”€â–¶â”‚ Evaluate â”‚â”€â”€â”€â–¶â”‚  Tune    â”‚â”€â”€â”€â–¶â”‚ Validate â”‚
 â”‚ Capture  â”‚    â”‚ Metrics  â”‚    â”‚ Params   â”‚    â”‚ No Regr. â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Loop â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
advanced-rag-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py              # LangChain Document Loaders (PDF, DOCX, TXT)
â”‚   â”‚   â””â”€â”€ processor.py           # LangChain RecursiveCharacterTextSplitter
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ embeddings.py          # LangChain HuggingFaceEmbeddings (bge-small-en-v1.5)
â”‚   â”‚   â”œâ”€â”€ vector_store.py        # LangChain Chroma VectorStore
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py      # LangChain BM25Retriever (sparse)
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py    # LangChain EnsembleRetriever (RRF fusion)
â”‚   â”‚   â””â”€â”€ reranker.py            # LangChain ContextualCompressionRetriever + CrossEncoder
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ llm.py                 # ChatHuggingFace / ChatOllama (dual backend)
â”‚   â”‚   â”œâ”€â”€ chain.py               # LCEL RAG Chain (prompt | llm | parser)
â”‚   â”‚   â””â”€â”€ prompts.py             # ChatPromptTemplate definitions
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py                # FastAPI REST + streaming endpoints
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py              # Structured logging (loguru)
â”‚       â””â”€â”€ cache.py               # Query result caching
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_data.py           # ArXiv paper downloader
â”‚   â”œâ”€â”€ build_index.py             # Build ChromaDB + BM25 indices
â”‚   â”œâ”€â”€ generate_eval_dataset.py   # Auto-generate evaluation queries
â”‚   â”œâ”€â”€ tune_parameters.py         # Automated parameter grid search
â”‚   â””â”€â”€ failure_analysis.py        # Error pattern analysis
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_system.py             # End-to-end integration test
â”‚   â”œâ”€â”€ test_langchain_imports.py  # LangChain component smoke tests (20 tests)
â”‚   â”œâ”€â”€ test_baseline_metrics.py   # Baseline capture & regression detection
â”‚   â”œâ”€â”€ test_evaluate_retrieval.py # P@K, R@K, MRR evaluation
â”‚   â”œâ”€â”€ test_evaluate_generation.py# Faithfulness, relevance, hallucination
â”‚   â””â”€â”€ test_score_thresholds.py   # Score threshold optimization
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Source documents (PDFs)
â”‚   â”œâ”€â”€ processed/                 # BM25 index (pickle)
â”‚   â”œâ”€â”€ vector_db/                 # ChromaDB persistent storage
â”‚   â””â”€â”€ test/                      # Evaluation dataset (JSON)
â”œâ”€â”€ .env                           # HF_API_TOKEN, LLM_BACKEND config
â”œâ”€â”€ requirements.txt               # LangChain ecosystem dependencies
â””â”€â”€ results/                       # Baseline metrics & evaluation reports
```

---

## ğŸ”§ Configuration (`.env`)

```env
# LLM Backend (HuggingFace Inference API â€” default)
LLM_BACKEND=huggingface
HF_MODEL=meta-llama/Llama-3.1-8B-Instruct
HF_API_TOKEN=hf_xxxxx                          # https://huggingface.co/settings/tokens

# Embeddings (HuggingFace)
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# Reranker (HuggingFace CrossEncoder)
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Hybrid Retrieval Fusion
DENSE_WEIGHT=0.7
SPARSE_WEIGHT=0.3
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# Ollama Fallback (local, offline)
OLLAMA_MODEL=llama3.1:8b
OLLAMA_BASE_URL=http://localhost:11434
```

---

## ğŸ“– Documentation

| Guide | Description |
|-------|-------------|
| [GETTING_STARTED.md](GETTING_STARTED.md) | Full setup â†’ testing â†’ tuning â†’ production workflow |
| [PARAMETER_TUNING.md](PARAMETER_TUNING.md) | Systematic parameter tuning & evaluation methodology |
| [API_REFERENCE.md](API_REFERENCE.md) | REST API endpoints, request/response examples & streaming |
| [TROUBLESHOOTING.md](TROUBLESHOOTING.md) | Common issues & solutions |

---

## ğŸ¯ Target vs Achieved Performance

| Metric | Target | Achieved | Status |
|--------|--------|----------|--------|
| Precision@5 | > 0.85 | 0.850 | âœ… |
| Answer Faithfulness | > 0.90 | 0.975 | âœ… |
| Hallucination Rate | < 0.05 | 0.025 | âœ… |
| Response Time (p95) | < 8s | 66ms | âœ… |
| Memory Usage | < 8GB | ~4GB | âœ… |

---

## License

MIT License
