<p align="center">
  <h1 align="center">ğŸ§  Advanced RAG System</h1>
  <p align="center">
    <strong>Production-grade Retrieval-Augmented Generation built entirely on LangChain + HuggingFace</strong>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/LangChain-LCEL_Pipeline-blue?logo=langchain" alt="LangChain"/>
    <img src="https://img.shields.io/badge/HuggingFace-Inference_API-yellow?logo=huggingface" alt="HuggingFace"/>
    <img src="https://img.shields.io/badge/LLM-Llama_3.1_8B-purple" alt="Llama 3.1"/>
    <img src="https://img.shields.io/badge/Embeddings-BGE--small-orange" alt="BGE"/>
    <img src="https://img.shields.io/badge/VectorDB-ChromaDB-green" alt="ChromaDB"/>
    <img src="https://img.shields.io/badge/API-FastAPI-009688?logo=fastapi" alt="FastAPI"/>
    <img src="https://img.shields.io/badge/LLMOps-Eval_+_Tuning-red" alt="LLMOps"/>
  </p>
</p>

An end-to-end RAG system following **LLMOps best practices**: every component is orchestrated via **LangChain**, every model is served through **HuggingFace** (embeddings, reranker, LLM), and the full lifecycle â€” ingestion, indexing, retrieval, generation, evaluation, tuning â€” is covered with production tooling.

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph INGEST["ğŸ“¥ INGESTION â€” LangChain Document Loaders"]
        direction TB
        A1["ğŸ“„ Raw Documents<br/><b>PDF Â· DOCX Â· TXT</b>"]
        A2["ğŸ”— LangChain Loaders<br/><code>PyPDFLoader</code> Â· <code>TextLoader</code> Â· <code>Docx2txtLoader</code>"]
        A3["âœ‚ï¸ LangChain Splitter<br/><code>RecursiveCharacterTextSplitter</code><br/>chunk_size=512 Â· overlap=50"]
        A1 --> A2 --> A3
    end

    subgraph INDEX["ğŸ—„ï¸ INDEXING â€” Dual Index (Dense + Sparse)"]
        direction TB
        B1["ğŸ§  HuggingFace Embeddings<br/><b>BAAI/bge-small-en-v1.5</b><br/><code>langchain_huggingface.HuggingFaceEmbeddings</code>"]
        B2["ğŸ“Š ChromaDB Vector Store<br/><code>langchain_chroma.Chroma</code><br/>384-dim Â· cosine similarity"]
        B3["ğŸ“ BM25 Sparse Index<br/><code>langchain_community.BM25Retriever</code><br/>TF-IDF keyword matching"]
    end

    subgraph RETRIEVE["ğŸ” RETRIEVAL â€” LangChain Hybrid Pipeline"]
        direction TB
        C1["ğŸ¯ Dense Retriever<br/><code>Chroma.as_retriever()</code>"]
        C2["ğŸ“‹ Sparse Retriever<br/><code>BM25Retriever</code>"]
        C3["âš¡ Ensemble Fusion<br/><code>EnsembleRetriever</code><br/>Reciprocal Rank Fusion<br/>dense=0.7 Â· sparse=0.3"]
        C4["ğŸ† Reranking<br/><code>ContextualCompressionRetriever</code><br/><b>+ CrossEncoderReranker</b><br/>ms-marco-MiniLM-L-6-v2"]
    end

    subgraph GENERATE["ğŸ¤– GENERATION â€” LCEL Chain (LangChain)"]
        direction TB
        D1["ğŸ“ <code>ChatPromptTemplate</code><br/>System + Context + Query"]
        D2["ğŸ¦™ <b>HuggingFace Inference API</b><br/><code>ChatHuggingFace</code> + <code>HuggingFaceEndpoint</code><br/>meta-llama/Llama-3.1-8B-Instruct"]
        D3["ğŸ“¤ <code>StrOutputParser</code>"]
        D4["ğŸ’¬ Answer"]
    end

    subgraph EVALUATE["ğŸ“Š LLMOps â€” EVALUATE & TUNE"]
        direction LR
        E1["ğŸ¯ Retrieval<br/>P@K Â· R@K Â· MRR"]
        E2["âœ… Generation<br/>Faithfulness<br/>Hallucination"]
        E3["ğŸ”§ Tuning<br/>Grid Search<br/>Failure Analysis"]
        E4["ğŸ“ˆ Baseline<br/>Tracking<br/>Regression Guard"]
    end

    subgraph SERVE["ğŸŒ SERVING â€” FastAPI"]
        direction LR
        F1["âš¡ REST API"]
        F2["ğŸ”„ Streaming"]
        F3["â¤ï¸ /health"]
        F4["ğŸ“š /docs"]
    end

    A3 --> B1 --> B2
    A3 --> B3
    B2 --> C1
    B3 --> C2
    C1 --> C3
    C2 --> C3
    C3 --> C4
    C4 --> D1 --> D2 --> D3 --> D4
    D4 --> F1

    RETRIEVE -.->|metrics| E1
    GENERATE -.->|metrics| E2
    E1 --> E3
    E2 --> E3
    E3 --> E4
    E4 -.->|iterate| RETRIEVE

    Q["ğŸ‘¤ User Query"] --> C1
    Q --> C2

    style INGEST fill:#1a1a2e,stroke:#e94560,color:#fff
    style INDEX fill:#16213e,stroke:#0f3460,color:#fff
    style RETRIEVE fill:#0f3460,stroke:#533483,color:#fff
    style GENERATE fill:#533483,stroke:#e94560,color:#fff
    style EVALUATE fill:#2d2d44,stroke:#e94560,color:#fff
    style SERVE fill:#1a1a2e,stroke:#0f3460,color:#fff
```

### Core LCEL Pipeline (LangChain Expression Language)

```python
# The entire RAG chain in one composable, streamable expression
chain = (
    {"context": retriever | format_docs, "query": RunnablePassthrough()}
    | ChatPromptTemplate.from_messages([system_msg, human_msg])
    | ChatHuggingFace(llm=HuggingFaceEndpoint(repo_id="meta-llama/Llama-3.1-8B-Instruct"))
    | StrOutputParser()
)
answer = chain.invoke("What is retrieval-augmented generation?")
```

---

## ğŸ§© LangChain Ã— HuggingFace â€” Components Map

Every module maps to a **LangChain** component. All models are served via **HuggingFace**:

| Layer | Module | LangChain Component | HuggingFace Model |
|-------|--------|--------------------|--------------------|
| **Ingestion** | `src/data/loader.py` | `PyPDFLoader` Â· `TextLoader` Â· `Docx2txtLoader` | â€” |
| **Ingestion** | `src/data/processor.py` | `RecursiveCharacterTextSplitter` | â€” |
| **Indexing** | `src/retrieval/embeddings.py` | `HuggingFaceEmbeddings` | `BAAI/bge-small-en-v1.5` |
| **Indexing** | `src/retrieval/vector_store.py` | `langchain_chroma.Chroma` | â€” |
| **Retrieval** | `src/retrieval/bm25_retriever.py` | `BM25Retriever` | â€” |
| **Retrieval** | `src/retrieval/hybrid_retriever.py` | `EnsembleRetriever` (RRF) | â€” |
| **Retrieval** | `src/retrieval/reranker.py` | `ContextualCompressionRetriever` + `CrossEncoderReranker` | `cross-encoder/ms-marco-MiniLM-L-6-v2` |
| **Generation** | `src/generation/llm.py` | `ChatHuggingFace` + `HuggingFaceEndpoint` | `meta-llama/Llama-3.1-8B-Instruct` |
| **Generation** | `src/generation/chain.py` | LCEL: `RunnablePassthrough` â†’ `PromptTemplate` â†’ `StrOutputParser` | â€” |
| **Generation** | `src/generation/prompts.py` | `ChatPromptTemplate.from_messages()` | â€” |
| **Serving** | `src/api/main.py` | FastAPI | â€” |

### 3 HuggingFace Models Used

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HuggingFace Model Hub                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Embeddings   â”‚  Reranking                â”‚  LLM (Generation)    â”‚
â”‚               â”‚                           â”‚                      â”‚
â”‚  BAAI/        â”‚  cross-encoder/           â”‚  meta-llama/         â”‚
â”‚  bge-small-   â”‚  ms-marco-               â”‚  Llama-3.1-8B-       â”‚
â”‚  en-v1.5      â”‚  MiniLM-L-6-v2           â”‚  Instruct            â”‚
â”‚               â”‚                           â”‚                      â”‚
â”‚  384-dim      â”‚  CrossEncoder             â”‚  Inference API       â”‚
â”‚  local CPU    â”‚  local CPU                â”‚  cloud (free tier)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LangChain:   â”‚  LangChain:              â”‚  LangChain:          â”‚
â”‚  HuggingFace  â”‚  CrossEncoderReranker    â”‚  ChatHuggingFace +   â”‚
â”‚  Embeddings   â”‚  + Contextual            â”‚  HuggingFaceEndpoint â”‚
â”‚               â”‚  CompressionRetriever    â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Quick Start

### Prerequisites

- Python 3.10+
- A free HuggingFace account â†’ [Get API token](https://huggingface.co/settings/tokens)

### Setup

```bash
# 1. Clone
git clone https://github.com/AMINAATTAOUI/advanced-rag-system.git
cd advanced-rag-system

# 2. Virtual environment
python -m venv .venv
.venv\Scripts\activate          # Windows
# source .venv/bin/activate     # Linux/Mac

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure HuggingFace token
cp .env.example .env
# Edit .env â†’ set HF_API_TOKEN=hf_your_token_here
```

### Run

```bash
# 5. Download documents (example: 50 ArXiv papers)
python scripts/download_data.py --num-papers 50

# 6. Build indices (ChromaDB vectors + BM25 sparse)
python scripts/build_index.py

# 7. Verify full pipeline
python tests/test_system.py

# 8. Start API
python -m uvicorn src.api.main:app --reload --port 8000
# â†’ Open http://localhost:8000/docs
```

### Query

```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the main contributions of this paper?", "top_k": 5}'
```

---

## ğŸ“Š LLMOps â€” Evaluation Pipeline

> **LLMOps best practice**: measure â†’ tune â†’ validate â†’ repeat.

### LLMOps Lifecycle

```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  BASELINE   â”‚â”€â”€â”€â”€â–¶â”‚  EVALUATE   â”‚â”€â”€â”€â”€â–¶â”‚    TUNE     â”‚â”€â”€â”€â”€â–¶â”‚  VALIDATE   â”‚
   â”‚  Capture    â”‚     â”‚  Retrieval  â”‚     â”‚  chunk_size â”‚     â”‚  No regres- â”‚
   â”‚  metrics    â”‚     â”‚  Generation â”‚     â”‚  weights    â”‚     â”‚  sion check â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  thresholds â”‚     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â–²                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ iterate â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Baseline Metrics (achieved)

| Category | Metric | Score | Assessment |
|----------|--------|-------|------------|
| **Retrieval** | Precision@5 | **0.850** | âœ… EXCELLENT |
| | Recall@5 | **0.900** | âœ… EXCELLENT |
| | MRR | **0.900** | âœ… EXCELLENT |
| | Latency (p95) | **66ms** | âš¡ FAST |
| **Generation** | Faithfulness | **0.975** | âœ… EXCELLENT |
| | Answer Relevance | **0.824** | âœ… GOOD |
| | Hallucination Rate | **0.025** | âœ… EXCELLENT |

### Run Evaluation

```bash
# 1. Capture baseline
python tests/test_baseline_metrics.py --dataset_size 50 --save_results

# 2. Retrieval evaluation â€” compare strategies
python tests/test_evaluate_retrieval.py --method dense --top_k 5
python tests/test_evaluate_retrieval.py --method sparse --top_k 5
python tests/test_evaluate_retrieval.py --method hybrid --top_k 5

# 3. Generation quality
python tests/test_evaluate_generation.py --method hybrid --num_queries 20

# 4. Tune parameters
python scripts/tune_parameters.py --param chunk_size --values 256,512,1024
python scripts/failure_analysis.py --threshold 0.6
python tests/test_score_thresholds.py --thresholds 0.3,0.5,0.7
```

---

## ğŸ“ Project Structure

```
advanced-rag-system/
â”‚
â”œâ”€â”€ src/                                    # â”€â”€ Source Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py                       # LangChain Document Loaders
â”‚   â”‚   â””â”€â”€ processor.py                    # LangChain RecursiveCharacterTextSplitter
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ embeddings.py                   # HuggingFaceEmbeddings (bge-small-en-v1.5)
â”‚   â”‚   â”œâ”€â”€ vector_store.py                 # LangChain Chroma VectorStore
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py               # LangChain BM25Retriever (sparse)
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py             # LangChain EnsembleRetriever (RRF)
â”‚   â”‚   â””â”€â”€ reranker.py                     # ContextualCompressionRetriever + CrossEncoder
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ llm.py                          # ChatHuggingFace + HuggingFaceEndpoint
â”‚   â”‚   â”œâ”€â”€ chain.py                        # LCEL Chain: prompt | llm | parser
â”‚   â”‚   â””â”€â”€ prompts.py                      # ChatPromptTemplate definitions
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py                         # FastAPI (REST + streaming + health)
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py                       # Structured logging (loguru)
â”‚       â””â”€â”€ cache.py                        # Query result caching
â”‚
â”œâ”€â”€ scripts/                                # â”€â”€ Automation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚   â”œâ”€â”€ download_data.py                    # Download ArXiv papers
â”‚   â”œâ”€â”€ build_index.py                      # Build ChromaDB + BM25 indices
â”‚   â”œâ”€â”€ generate_eval_dataset.py            # Auto-generate eval queries from corpus
â”‚   â”œâ”€â”€ tune_parameters.py                  # Parameter grid search
â”‚   â””â”€â”€ failure_analysis.py                 # Error pattern analysis
â”‚
â”œâ”€â”€ tests/                                  # â”€â”€ Evaluation Suite â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚   â”œâ”€â”€ test_system.py                      # End-to-end integration test
â”‚   â”œâ”€â”€ test_langchain_imports.py           # LangChain smoke tests (20 tests)
â”‚   â”œâ”€â”€ test_baseline_metrics.py            # Baseline capture & regression guard
â”‚   â”œâ”€â”€ test_evaluate_retrieval.py          # P@K, R@K, MRR by method
â”‚   â”œâ”€â”€ test_evaluate_generation.py         # Faithfulness, relevance, hallucination
â”‚   â””â”€â”€ test_score_thresholds.py            # Threshold optimization
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                                # Source documents (PDFs)
â”‚   â”œâ”€â”€ processed/                          # BM25 index (pickle)
â”‚   â”œâ”€â”€ vector_db/                          # ChromaDB persistent storage
â”‚   â””â”€â”€ test/                               # Evaluation dataset (JSON)
â”‚
â”œâ”€â”€ results/                                # Evaluation reports & baselines
â”œâ”€â”€ .env                                    # HF_API_TOKEN configuration
â”œâ”€â”€ .env.example                            # Template for .env
â””â”€â”€ requirements.txt                        # Dependencies (LangChain + HuggingFace)
```

---

## ğŸ”§ Configuration

All configuration is via environment variables (`.env` file):

```env
# HuggingFace Inference API
HF_API_TOKEN=hf_xxxxx               # Get from https://huggingface.co/settings/tokens
HF_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Embeddings
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# Reranker
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Retrieval
DENSE_WEIGHT=0.7                     # Hybrid fusion weights
SPARSE_WEIGHT=0.3
CHUNK_SIZE=512                       # Text splitter chunk size
CHUNK_OVERLAP=50
```

---

## ğŸ“– Documentation

| Guide | Description |
|-------|-------------|
| [GETTING_STARTED.md](GETTING_STARTED.md) | Full setup â†’ testing â†’ tuning â†’ production |
| [PARAMETER_TUNING.md](PARAMETER_TUNING.md) | Evaluation methodology & parameter optimization |
| [API_REFERENCE.md](API_REFERENCE.md) | REST API endpoints, examples & streaming |
| [TROUBLESHOOTING.md](TROUBLESHOOTING.md) | Common issues & solutions |

---

## ğŸ¯ Performance Summary

| Metric | Target | Achieved |
|--------|--------|----------|
| Precision@5 | > 0.85 | **0.850** âœ… |
| Recall@5 | > 0.85 | **0.900** âœ… |
| Faithfulness | > 0.90 | **0.975** âœ… |
| Hallucination | < 0.05 | **0.025** âœ… |
| Latency (p95) | < 8s | **66ms** âœ… |

---

## License

MIT License
